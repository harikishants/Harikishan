{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6c644737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1d3990f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test = 'reviews_test.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1c1ad8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "def read_texts_and_labels(file, max_lines):\n",
    "  texts = []\n",
    "  labels = []\n",
    "  num_lines=0\n",
    "  for line in bz2.BZ2File(file):\n",
    "    x = line.decode(\"utf-8\")\n",
    "    label, text = x.split(\" \", 1)\n",
    "    #print(text)\n",
    "    # Convert positive sentiment labels to 1 and negative sentiment labels to 0\n",
    "    if label == \"__label__1\":\n",
    "        label = 0\n",
    "    elif label == \"__label__2\":\n",
    "        label = 1\n",
    "    texts.append(text.strip())\n",
    "    labels.append(label)\n",
    "    num_lines += 1\n",
    "    if(num_lines >= max_lines): break\n",
    "  #texts = np.array(texts)\n",
    "  labels = np.array(labels)\n",
    "  print(num_lines)\n",
    "  return texts, labels\n",
    "\n",
    "max_lines_test = 50\n",
    "test_texts, test_labels = read_texts_and_labels(filename_test, max_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b68283d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "with open(\"train_texts.txt\", \"r\") as f:\n",
    "  for line in f:\n",
    "    train_texts.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5d80dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = 3000 # you may experiment with different numbers\n",
    "tokenizer = Tokenizer(num_words=num_of_words)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "max_length_of_sequences = 235 # Obtained while model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "89592c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length_of_sequences, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0a965598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN = tf.keras.models.load_model('Model_NN_100000.h5')\n",
    "model_GRU = tf.keras.models.load_model('Model_GRU_100000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "5e6f46f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3186c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    #print(\"inside\")\n",
    "    # Encode the text\n",
    "    encoded_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # Predict the sentiment\n",
    "    sentiment = model(encoded_text)[0]\n",
    "    # Decode the sentiment\n",
    "    #print(sentiment.argmax().item())\n",
    "    return sentiment.argmax().item()\n",
    "\n",
    "def gpt2_predict(test_texts):\n",
    "# Test the model\n",
    "    #print(\"here\")\n",
    "    labels = []\n",
    "    for text in test_texts:\n",
    "        #print(text)\n",
    "        \n",
    "        sentiment = predict_sentiment(text)\n",
    "        labels.append(sentiment)\n",
    "        #sentiment.append(predict_sentiment(text))\n",
    "        #print(\"Sentiment: \",sentiment) # 1 for positive, 0 for negative\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7abb546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def gpt3_predict(test_texts):\n",
    "    score = pipeline(\"sentiment-analysis\")\n",
    "    labels = []\n",
    "    for text in test_texts:\n",
    "        if score(text)[0]['label'] == \"POSITIVE\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "1461f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = '### USE YOUR OWN OPENAI API KEY'\n",
    "\n",
    "gpt3p5_labels = []\n",
    "def analyze_gpt35(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are trained to analyze and detect the sentiment of given text. \n",
    "                                        If you're unsure of an answer, you can say \"not sure\" and recommend users to review manually.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Analyze the following product review and determine if the sentiment is: positive or negative. \n",
    "                                        Return answer in single word as either positive or negative: {text}\"\"\"}\n",
    "        ]\n",
    "   \n",
    "    response = openai.ChatCompletion.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=messages, \n",
    "                      max_tokens=1, \n",
    "                      n=1, \n",
    "                      stop=None, \n",
    "                      temperature=0)\n",
    "\n",
    "    response_text = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "def gpt3p5_predict(test_texts):\n",
    "    for text in test_texts:\n",
    "        out = analyze_gpt35(text)\n",
    "        print(out)\n",
    "        if out == \"positive\":\n",
    "            gpt3p5_labels.append(1)\n",
    "        else:\n",
    "            gpt3p5_labels.append(0)\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "235d3589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 1s 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Implement code here\n",
    "test_pred_ffn = model_NN.predict(test_sequences)\n",
    "test_pred_gru = model_GRU.predict(test_sequences)\n",
    "test_pred_gpt2 = np.array(gpt2_predict(test_texts))\n",
    "test_pred_gpt3 = np.array(gpt3_predict(test_texts))\n",
    "\n",
    "gpt3p5_predict(test_texts)\n",
    "test_pred_gpt3p5 = np.array(gpt3p5_labels)\n",
    "#print(test_pred_gpt2)\n",
    "\n",
    "test_pred_labels_ffn = np.round(test_pred_ffn)\n",
    "test_pred_labels_gru = np.round(test_pred_gru)\n",
    "test_pred_labels_gpt2 = test_pred_gpt2.reshape(-1,1)\n",
    "test_pred_labels_gpt3 = test_pred_gpt3.reshape(-1,1)\n",
    "test_pred_labels_gpt3p5 = test_pred_gpt3p5.reshape(-1,1)\n",
    "\n",
    "#print(test_pred_labels_ffn)\n",
    "#print(test_pred_labels_gru)\n",
    "#print(test_pred_labels_gpt2)\n",
    "print(test_pred_labels_gpt3p5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6149ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN model accuracy: 0.880\n",
      "FFN model F1 score: 0.889\n",
      "GRU model accuracy: 0.900\n",
      "GRU model F1 score: 0.915\n",
      "GPT2 model accuracy: 0.440\n",
      "GPT2 model F1 score: 0.000\n",
      "GPT3 model accuracy: 0.860\n",
      "GPT3 model F1 score: 0.877\n",
      "GPT3.5 model accuracy: 0.980\n",
      "GPT3.5 model F1 score: 0.982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "acc_ffn = accuracy_score(test_labels, test_pred_labels_ffn)\n",
    "f1_ffn = f1_score(test_labels, test_pred_labels_ffn)\n",
    "\n",
    "acc_gru = accuracy_score(test_labels, test_pred_labels_gru)\n",
    "f1_gru = f1_score(test_labels, test_pred_labels_gru)\n",
    "\n",
    "acc_gpt2 = accuracy_score(test_labels, test_pred_labels_gpt2)\n",
    "f1_gpt2 = f1_score(test_labels, test_pred_labels_gpt2)\n",
    "\n",
    "acc_gpt3 = accuracy_score(test_labels, test_pred_labels_gpt3)\n",
    "f1_gpt3 = f1_score(test_labels, test_pred_labels_gpt3)\n",
    "\n",
    "acc_gpt3p5 = accuracy_score(test_labels, test_pred_labels_gpt3p5)\n",
    "f1_gpt3p5 = f1_score(test_labels, test_pred_labels_gpt3p5)\n",
    "\n",
    "print(\"FFN model accuracy: {:.3f}\".format(acc_ffn))\n",
    "print(\"FFN model F1 score: {:.3f}\".format(f1_ffn))\n",
    "\n",
    "print(\"GRU model accuracy: {:.3f}\".format(acc_gru))\n",
    "print(\"GRU model F1 score: {:.3f}\".format(f1_gru))\n",
    "\n",
    "print(\"GPT2 model accuracy: {:.3f}\".format(acc_gpt2))\n",
    "print(\"GPT2 model F1 score: {:.3f}\".format(f1_gpt2))\n",
    "\n",
    "print(\"GPT3 model accuracy: {:.3f}\".format(acc_gpt3))\n",
    "print(\"GPT3 model F1 score: {:.3f}\".format(f1_gpt3))\n",
    "\n",
    "print(\"GPT3.5 model accuracy: {:.3f}\".format(acc_gpt3p5))\n",
    "print(\"GPT3.5 model F1 score: {:.3f}\".format(f1_gpt3p5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc074f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

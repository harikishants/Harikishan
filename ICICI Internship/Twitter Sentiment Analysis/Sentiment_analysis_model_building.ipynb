{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfcb3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31880521",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = 'reviews_train.bz2'\n",
    "filename_test = 'reviews_test.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6572ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "def read_texts_and_labels(file, max_lines):\n",
    "  texts = []\n",
    "  labels = []\n",
    "  num_lines=0\n",
    "  for line in bz2.BZ2File(file):\n",
    "    x = line.decode(\"utf-8\")\n",
    "    label, text = x.split(\" \", 1)\n",
    "    #print(text)\n",
    "    # Convert positive sentiment labels to 1 and negative sentiment labels to 0\n",
    "    if label == \"__label__1\":\n",
    "        label = 0\n",
    "    elif label == \"__label__2\":\n",
    "        label = 1\n",
    "    texts.append(text.strip())\n",
    "    labels.append(label)\n",
    "    num_lines += 1\n",
    "    if(num_lines >= max_lines): break\n",
    "  #texts = np.array(texts)\n",
    "  labels = np.array(labels)\n",
    "  print(num_lines)\n",
    "  return texts, labels\n",
    "\n",
    "max_lines_train = 100000\n",
    "max_lines_test = 4000\n",
    "train_texts, train_labels = read_texts_and_labels(filename_train, max_lines_train)\n",
    "test_texts, test_labels = read_texts_and_labels(filename_test, max_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f005b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28cc6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.05)\n",
    "#print(len(val_labels))\n",
    "#print(len(train_labels))\n",
    "print(type(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1ecaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_texts.txt\", \"w\") as f:\n",
    "    for x in train_texts:\n",
    "        f.write(str(x) +\"\\n\")\n",
    "        \n",
    "#score = []\n",
    "#with open(\"file.txt\", \"r\") as f:\n",
    " # for line in f:\n",
    "  #  score.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3457e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = 3000 # you may experiment with different numbers\n",
    "tokenizer = Tokenizer(num_words=num_of_words)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8a3ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_texts)\n",
    "del(val_texts)\n",
    "del(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f35afea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "#max_length_of_sequences = 200\n",
    "max_length_of_sequences = max(len(seq) for seq in train_sequences + val_sequences + test_sequences)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length_of_sequences, padding='pre')\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_length_of_sequences, padding='pre')\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length_of_sequences, padding='pre')\n",
    "print(max_length_of_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8be8e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may tune these hyperparameters, but implement the model as instructed above.\n",
    "d1 = 200 #Tuned\n",
    "d2 = 128 #Tuned\n",
    "\n",
    "def model_FFN():\n",
    "  model = keras.Sequential([\n",
    "  layers.Embedding(input_dim=num_of_words, output_dim=d1, input_length=max_length_of_sequences),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(d2, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "  return model # implement the model and return the model\n",
    "    \n",
    "model1 = model_FFN()\n",
    "learning_rate = 0.001 #Tuned\n",
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate), #Tuned\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8309ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "743/743 [==============================] - 38s 51ms/step - loss: 0.4937 - accuracy: 0.7359 - val_loss: 0.3616 - val_accuracy: 0.8346\n",
      "Epoch 2/10\n",
      "743/743 [==============================] - 37s 50ms/step - loss: 0.2795 - accuracy: 0.8841 - val_loss: 0.2995 - val_accuracy: 0.8768\n",
      "Epoch 3/10\n",
      "743/743 [==============================] - 37s 50ms/step - loss: 0.2160 - accuracy: 0.9143 - val_loss: 0.2979 - val_accuracy: 0.8846\n",
      "Epoch 4/10\n",
      "743/743 [==============================] - 37s 50ms/step - loss: 0.1350 - accuracy: 0.9518 - val_loss: 0.3639 - val_accuracy: 0.8734\n",
      "Epoch 5/10\n",
      "743/743 [==============================] - 37s 50ms/step - loss: 0.0674 - accuracy: 0.9778 - val_loss: 0.5008 - val_accuracy: 0.8610\n",
      "Epoch 6/10\n",
      "743/743 [==============================] - 37s 50ms/step - loss: 0.0296 - accuracy: 0.9919 - val_loss: 0.6464 - val_accuracy: 0.8536\n",
      "Epoch 7/10\n",
      "743/743 [==============================] - 39s 53ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.7517 - val_accuracy: 0.8648\n",
      "Epoch 8/10\n",
      "743/743 [==============================] - 38s 51ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.8578 - val_accuracy: 0.8640\n",
      "Epoch 9/10\n",
      "743/743 [==============================] - 38s 51ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.9646 - val_accuracy: 0.8660\n",
      "Epoch 10/10\n",
      "743/743 [==============================] - 38s 51ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.0257 - val_accuracy: 0.8658\n",
      "<keras.callbacks.History object at 0x7fd5dcd4dd50>\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    batch_size=128\n",
    ")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f70ac06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"Model_NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41fada22",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a9e47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 200\n",
    "d2 = 128\n",
    "d3 = 64 #tuned\n",
    "\n",
    "def model_GRU():\n",
    "  model = keras.Sequential([\n",
    "    layers.Embedding(num_of_words, d1, input_length=max_length_of_sequences),\n",
    "    layers.Bidirectional(layers.GRU(d2, return_sequences=True)),\n",
    "    layers.GRU(d3),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model2 = model_GRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b06b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model_GRU()\n",
    "learning_rate=0.001\n",
    "model2.compile(\n",
    "    optimizer='Adam',#tuned\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "954f7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "743/743 [==============================] - 393s 521ms/step - loss: 0.3305 - accuracy: 0.8590 - val_loss: 0.2345 - val_accuracy: 0.9038\n",
      "Epoch 2/10\n",
      "743/743 [==============================] - 388s 523ms/step - loss: 0.2027 - accuracy: 0.9209 - val_loss: 0.2167 - val_accuracy: 0.9104\n",
      "Epoch 3/10\n",
      "743/743 [==============================] - 384s 517ms/step - loss: 0.1768 - accuracy: 0.9322 - val_loss: 0.2069 - val_accuracy: 0.9204\n",
      "Epoch 4/10\n",
      "743/743 [==============================] - 382s 514ms/step - loss: 0.1571 - accuracy: 0.9408 - val_loss: 0.2139 - val_accuracy: 0.9184\n",
      "Epoch 5/10\n",
      "743/743 [==============================] - 383s 515ms/step - loss: 0.1371 - accuracy: 0.9491 - val_loss: 0.2220 - val_accuracy: 0.9130\n",
      "Epoch 6/10\n",
      "743/743 [==============================] - 395s 531ms/step - loss: 0.1178 - accuracy: 0.9578 - val_loss: 0.2380 - val_accuracy: 0.9156\n",
      "Epoch 7/10\n",
      "743/743 [==============================] - 386s 520ms/step - loss: 0.0976 - accuracy: 0.9651 - val_loss: 0.2594 - val_accuracy: 0.9112\n",
      "Epoch 8/10\n",
      "743/743 [==============================] - 381s 513ms/step - loss: 0.0810 - accuracy: 0.9722 - val_loss: 0.2788 - val_accuracy: 0.9096\n",
      "Epoch 9/10\n",
      "743/743 [==============================] - 372s 501ms/step - loss: 0.0656 - accuracy: 0.9777 - val_loss: 0.3147 - val_accuracy: 0.9062\n",
      "Epoch 10/10\n",
      "743/743 [==============================] - 377s 508ms/step - loss: 0.0537 - accuracy: 0.9821 - val_loss: 0.3328 - val_accuracy: 0.9088\n",
      "<keras.callbacks.History object at 0x7fd56e836020>\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    train_sequences,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_sequences, val_labels),\n",
    "    batch_size=128\n",
    ")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df5ded8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"Model_GRU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81850ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ced93b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Implement code here\\ntest_pred_ffn = model1.predict(test_sequences)\\ntest_pred_gru = model2.predict(test_sequences)\\n\\ntest_pred_labels_ffn = np.round(test_pred_ffn)\\ntest_pred_labels_gru = np.round(test_pred_gru)\\n\\nfrom sklearn.metrics import accuracy_score, f1_score\\n\\nacc_ffn = accuracy_score(test_labels, test_pred_labels_ffn)\\nf1_ffn = f1_score(test_labels, test_pred_labels_ffn)\\n\\nacc_gru = accuracy_score(test_labels, test_pred_labels_gru)\\nf1_gru = f1_score(test_labels, test_pred_labels_gru)\\n\\nprint(\"FFN model accuracy: {:.3f}\".format(acc_ffn))\\nprint(\"FFN model F1 score: {:.3f}\".format(f1_ffn))\\n\\nprint(\"GRU model accuracy: {:.3f}\".format(acc_gru))\\nprint(\"GRU model F1 score: {:.3f}\".format(f1_gru))'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Implement code here\n",
    "test_pred_ffn = model1.predict(test_sequences)\n",
    "test_pred_gru = model2.predict(test_sequences)\n",
    "\n",
    "test_pred_labels_ffn = np.round(test_pred_ffn)\n",
    "test_pred_labels_gru = np.round(test_pred_gru)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "acc_ffn = accuracy_score(test_labels, test_pred_labels_ffn)\n",
    "f1_ffn = f1_score(test_labels, test_pred_labels_ffn)\n",
    "\n",
    "acc_gru = accuracy_score(test_labels, test_pred_labels_gru)\n",
    "f1_gru = f1_score(test_labels, test_pred_labels_gru)\n",
    "\n",
    "print(\"FFN model accuracy: {:.3f}\".format(acc_ffn))\n",
    "print(\"FFN model F1 score: {:.3f}\".format(f1_ffn))\n",
    "\n",
    "print(\"GRU model accuracy: {:.3f}\".format(acc_gru))\n",
    "print(\"GRU model F1 score: {:.3f}\".format(f1_gru))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de92b78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd4f74c6f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Output NN [[0.01717249]]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd4f74c7a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 788ms/step\n",
      "Output GRU [[0.0003938]]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"This is not good\"\n",
    "mytest = tokenizer.texts_to_sequences([test_text])\n",
    "mytest = pad_sequences(mytest, maxlen=max_length_of_sequences, padding='pre')\n",
    "new_model_NN = tf.keras.models.load_model('Model_NN.h5')\n",
    "new_model_GRU = tf.keras.models.load_model('Model_GRU.h5')\n",
    "print(\"Output NN\", new_model_NN.predict(mytest))\n",
    "print(\"Output GRU\", new_model_GRU.predict(mytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a417a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
